{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":74},"id":"F358oYmUX5E4","outputId":"99b7281c-82fb-4b01-c81e-49cda7690f44"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'google'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m files\n\u001b[0;32m      3\u001b[0m uploades \u001b[38;5;241m=\u001b[39m files\u001b[38;5;241m.\u001b[39mupload()\n","\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"]}],"source":["from google.colab import files\n","\n","uploades = files.upload()"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"DXkDrHWjYCXN"},"outputs":[],"source":["import scipy\n","\n","import scipy.io\n","\n","# Load the MATLAB file\n","\n","mat_data = scipy.io.loadmat('HW3Data.mat')\n","\n","# Access variables in the MATLAB file\n","# For example, if you have a variable 'data' in the MATLAB file:\n","# data = mat_data['data']\n","# Now 'data' in Python contains the data from the 'data' variable in the MATLAB file.\n","\n","Vocabulary = mat_data['Vocabulary']\n","XTrain = mat_data['XTrain']\n","XTest = mat_data['XTest']\n","yTrain = mat_data['yTrain']\n","yTest = mat_data['yTest']\n","XTrainSmall = mat_data['XTrainSmall']\n","yTrainSmall = mat_data['yTrainSmall']\n","\n","XTrain = XTrain.toarray()\n","XTest = XTest.toarray()\n","XTrainSmall = XTrainSmall.toarray()"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"gVPClhuOlAeE"},"outputs":[],"source":["import numpy as np\n","\n","def logProd(x):\n","    # x is vector of logs of probabilities\n","    # logprod returns the log of the products of the probabilites mentioned above\n","    log_product = np.sum(x)      # TODO: update definition\n","    return log_product\n"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"RR_nAVztWoYc"},"outputs":[],"source":["\n","import numpy as np\n","\n","def NB_XGivenY(XTrain, yTrain):\n","    # XTrain: n x V matrix\n","    # yTrain: n x 1 vector\n","    V = XTrain.shape[1]\n","    D = np.zeros((2, V))\n","    for y in [1, 2]:\n","        docs_in_class = XTrain[yTrain[:, 0] == y]\n","        D[y-1, :] = (np.sum(docs_in_class, axis=0) + 2) / (docs_in_class.shape[0] + 3)\n","    return D\n","\n","\n","def NB_YPrior(yTrain):\n","    num_economist = np.sum(yTrain == 1)\n","    num_docs = len(yTrain)\n","    return num_economist / num_docs\n","\n"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"wbM-0-NSk5X5"},"outputs":[],"source":["import numpy as np\n","\n","\n","def NB_Classify(D, p, X):\n","    log_D = np.log(D)\n","    log_1_minus_D = np.log(1 - D)\n","    log_prior = np.log([p, 1-p])\n","    \n","    yHat = np.zeros(X.shape[0], dtype=int)\n","    for i in range(X.shape[0]):\n","        log_p_economist = np.sum(log_D[0, :] * X[i, :] + log_1_minus_D[0, :] * (1 - X[i, :])) + log_prior[0]\n","        log_p_onion = np.sum(log_D[1, :] * X[i, :] + log_1_minus_D[1, :] * (1 - X[i, :])) + log_prior[1]\n","        yHat[i] = 1 if log_p_economist > log_p_onion else 2\n","    return yHat\n"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"guZJZnPAkpHy"},"outputs":[],"source":["import numpy as np\n","\n","def ClassificationError(yHat, yTruth):\n","    return np.mean(yHat != yTruth)\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"8l1ONLwuZKh7"},"outputs":[],"source":["D = NB_XGivenY(XTrain, yTrain)\n","p = NB_YPrior(yTrain)\n"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"lxLQjSiKClM8"},"outputs":[],"source":["yHatTrain = NB_Classify(D, p, XTrain)\n","yHatTest = NB_Classify(D, p, XTest)"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"lQGSJ4b1zaLn"},"outputs":[],"source":["\n","trainError = ClassificationError(yHatTrain, yTrain)\n","testError = ClassificationError(yHatTest, yTest)"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"NaWt_BZnllyM"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training Error: 0.3102259215219976\n","Testing Error: 0.298359096313912\n"]}],"source":["print(\"Training Error:\", trainError)\n","print(\"Testing Error:\", testError)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"sYWueoT45tvQ"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.3102259215219976\n","0.298359096313912\n"]}],"source":["yHatTest = NB_Classify(D, p, XTest)\n","trainError = ClassificationError(yHatTrain, yTrainSmall)\n","testError = ClassificationError(yHatTest, yTest)\n","print(trainError)\n","print(testError)\n"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Training Error (Small): 0.25862068965517243\n","Testing Error (Small): 0.2896551724137931\n"]}],"source":["D_small = NB_XGivenY(XTrainSmall, yTrainSmall)\n","p_small = NB_YPrior(yTrainSmall)\n","\n","yHatTrainSmall = NB_Classify(D_small, p_small, XTrainSmall)\n","yHatTestSmall = NB_Classify(D_small, p_small, XTest)\n","\n","trainErrorSmall = ClassificationError(yHatTrainSmall, yTrainSmall)\n","testErrorSmall = ClassificationError(yHatTestSmall, yTest)\n","\n","print(\"Training Error (Small):\", trainErrorSmall)\n","print(\"Testing Error (Small):\", testErrorSmall)\n"]},{"cell_type":"markdown","metadata":{},"source":["##### With the smaller training set, the training error decreases slightly (from 0.31 to 0.26) due to potential overfitting, where the model may memorize the smaller dataset more easily. However, the test error remains stable at 0.29 for both the full and small datasets. This indicates that despite having less data, the classifier's performance on unseen data doesn't significantly worsen. The prior has a greater impact with less training data, making the model rely more on prior assumptions, but overall, the model's generalization ability remains largely unaffected.\n"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Top 5 words for The Economist:\n","['a']\n","['of']\n","['in']\n","['the']\n","['to']\n","\n","Top 5 words for The Onion:\n","['to']\n","['in']\n","['a']\n","['said']\n","['of']\n","\n","Top 5 discriminative words for The Economist:\n","['parliament']\n","['centr']\n","['favour']\n","['reckon']\n","['organis']\n","\n","Top 5 discriminative words for The Onion:\n","['tuesday']\n","['percent']\n","['monday']\n","['5enlarg']\n","['4enlarg']\n"]}],"source":["def interpret_words(D, Vocabulary):\n","     \n","    top_5_class1 = np.argsort(D[0, :])[-5:]  # For class 1 (The Economist)\n","    top_5_class2 = np.argsort(D[1, :])[-5:]  # For class 2 (The Onion)\n","    \n","    print(\"Top 5 words for The Economist:\")\n","    for idx in top_5_class1:\n","        print(Vocabulary[idx, 0])\n","\n","    print(\"\\nTop 5 words for The Onion:\")\n","    for idx in top_5_class2:\n","        print(Vocabulary[idx, 0])\n","\n","     \n","    ratio_class1 = D[0, :] / D[1, :]\n","    ratio_class2 = D[1, :] / D[0, :]\n","    \n","    top_5_discriminative_class1 = np.argsort(ratio_class1)[-5:]\n","    top_5_discriminative_class2 = np.argsort(ratio_class2)[-5:]\n","\n","    print(\"\\nTop 5 discriminative words for The Economist:\")\n","    for idx in top_5_discriminative_class1:\n","        print(Vocabulary[idx, 0])\n","\n","    print(\"\\nTop 5 discriminative words for The Onion:\")\n","    for idx in top_5_discriminative_class2:\n","        print(Vocabulary[idx, 0])\n","\n","\n","interpret_words(D, Vocabulary)\n"]},{"cell_type":"markdown","metadata":{},"source":["##### The discriminative words better describe the two classes because they reflect specific content related to each magazine. The Economist features formal words like \"parliament\" and \"favour,\" while The Onion uses casual terms like \"tuesday\" and \"percent,\" which align with its satirical style. In contrast, the common top 5 words are mostly stop words like \"a\" and \"the,\" which don't offer much insight into the differences between the two publications."]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.2"},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat":4,"nbformat_minor":0}
